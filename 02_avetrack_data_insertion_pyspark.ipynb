{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a70a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DELTA_TABLE_PATH = \"abfss://datacare@onelake.dfs.fabric.microsoft.com/mobileapps.Lakehouse/Tables/att/avetrack_partitioned\"\n",
    "MONGODB_DATABASE = \"avetrack_docdb\"\n",
    "MONGODB_COLLECTION = \"orders\"\n",
    "conn_str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e1193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"âœ… Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from Delta Lake\n",
    "delta_df = spark.read.format(\"delta\").load(DELTA_TABLE_PATH)\n",
    "record_count = delta_df.count()\n",
    "\n",
    "print(f\"âœ… Loaded {record_count:,} records\")\n",
    "delta_df.select(\"_id\", \"fields.run\", \"serviceId\", \"version\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-adjust batch size based on record count\n",
    "if record_count <= 20000000:\n",
    "    BATCH_SIZE = \"2000\"\n",
    "elif record_count <= 50000000:\n",
    "    BATCH_SIZE = \"5000\"\n",
    "elif record_count <= 100000000:\n",
    "    BATCH_SIZE = \"8000\"\n",
    "else:\n",
    "    BATCH_SIZE = \"10000\"\n",
    "\n",
    "print(f\"ðŸ“¦ Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fde5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition for optimal parallel writes to MongoDB\n",
    "# Rule: 128-256 MB per partition for best performance\n",
    "# Current: 4 partitions of ~100GB each (very slow!)\n",
    "# Target: 1500-2000 partitions of ~200MB each (fast parallel writes)\n",
    "\n",
    "current_partitions = delta_df.rdd.getNumPartitions()\n",
    "WRITE_PARTITIONS = 2000  # Adjust based on cluster size\n",
    "\n",
    "print(f\"âš ï¸  Current partitions: {current_partitions}\")\n",
    "print(f\"ðŸ”„ Repartitioning to {WRITE_PARTITIONS} for parallel writes...\")\n",
    "\n",
    "delta_df = delta_df.repartition(WRITE_PARTITIONS)\n",
    "\n",
    "print(f\"âœ… Repartitioned: Each task will write ~{record_count//WRITE_PARTITIONS:,} records\")\n",
    "print(f\"ðŸ“Š Parallel tasks: {WRITE_PARTITIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data into Azure DocumentDB\n",
    "if not conn_str:\n",
    "    raise ValueError(\"âŒ Connection string not set\")\n",
    "\n",
    "print(f\"ðŸš€ Inserting {record_count:,} records...\")\n",
    "\n",
    "delta_df.write.format(\"mongodb\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option('connection.uri', conn_str) \\\n",
    "    .option(\"database\", MONGODB_DATABASE) \\\n",
    "    .option(\"collection\", MONGODB_COLLECTION) \\\n",
    "    .option(\"convertJson\", \"true\") \\\n",
    "    .option(\"idFieldList\", \"_id,fields.run\") \\\n",
    "    .option(\"ignoreNullValues\", \"true\") \\\n",
    "    .option(\"maxBatchSize\", BATCH_SIZE) \\\n",
    "    .option(\"operationType\", \"update\") \\\n",
    "    .option(\"ordered\", \"true\") \\\n",
    "    .option(\"upsertDocument\", \"true\") \\\n",
    "    .save()\n",
    "\n",
    "print(f\"âœ… Inserted {record_count:,} records\")\n",
    "print(f\"ðŸ—„ï¸  Database: {MONGODB_DATABASE}.{MONGODB_COLLECTION}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
