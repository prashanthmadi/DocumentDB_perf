{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e76e1e",
   "metadata": {},
   "source": [
    "# ðŸ“Š Avetrack Application Data Generator - 116KB Documents\n",
    "\n",
    "Generate large-scale avetrack-style application data matching customer's **116KB document size** for DocumentDB performance testing.\n",
    "\n",
    "## ðŸŽ¯ Document Size Target:\n",
    "- **~116 KB per document** (matches customer sample: 118,580 bytes raw)\n",
    "- **Customer scenario**: 4TB collection = ~37M documents\n",
    "\n",
    "## ðŸ“¦ Size Components:\n",
    "| Component | Size | Description |\n",
    "|-----------|------|-------------|\n",
    "| **WPP fields** | ~20 KB | 30+ identity verification fields |\n",
    "| **losgs array** | ~40-50 KB | 10-15 lineItems with full price/tax details |\n",
    "| **neustar data** | ~15-20 KB | Nested fraud detection responses |\n",
    "| **Base fields** | ~25-30 KB | checkOutList, orderList, payments, shipping |\n",
    "| **Total** | **~116 KB** | âœ… Matches customer sample |\n",
    "\n",
    "## ðŸ“Š Scaling:\n",
    "- **1M records** = ~116 GB\n",
    "- **10M records** = ~1.16 TB\n",
    "- **37M records** = ~4.3 TB (customer target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Adjust these parameters as needed\n",
    "NUM_RECORDS = 1000000  # Start with 1M for testing (~116 GB total)\n",
    "NUM_PARTITIONS = 200\n",
    "DELTA_TABLE_PATH = \"abfss://datacare@onelake.dfs.fabric.microsoft.com/mobileapps.Lakehouse/Tables/att/avetrack_applications_large\"\n",
    "\n",
    "# Document size target\n",
    "TARGET_DOC_SIZE_KB = 116  # Matches customer sample (118,580 bytes)\n",
    "\n",
    "print(f\"ðŸŽ¯ Target: {TARGET_DOC_SIZE_KB} KB per document\")\n",
    "print(f\"ðŸ“Š Records: {NUM_RECORDS:,}\")\n",
    "print(f\"ðŸ’¾ Total size: {NUM_RECORDS * TARGET_DOC_SIZE_KB / 1024 / 1024:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-adjust partitions based on record count\n",
    "if NUM_RECORDS <= 1000000:\n",
    "    NUM_PARTITIONS = 100\n",
    "elif NUM_RECORDS <= 10000000:\n",
    "    NUM_PARTITIONS = 500\n",
    "else:\n",
    "    NUM_PARTITIONS = 1000\n",
    "\n",
    "print(f\"âš¡ Using {NUM_PARTITIONS} partitions\")\n",
    "print(f\"ðŸ“¦ Est. size: {NUM_RECORDS * TARGET_DOC_SIZE_KB / 1024 / 1024:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7541ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference data for realistic generation\n",
    "ALL_US_STATES = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \n",
    "                 \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \n",
    "                 \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\", \"DC\"]\n",
    "\n",
    "# Weighted distributions for realism\n",
    "WEIGHTED_STATES = [\"TX\"] * 15 + [\"CA\"] * 15 + [\"NY\"] * 10 + [\"FL\"] * 10 + ALL_US_STATES * 2\n",
    "RUN_TYPES = [\"RUN2\"] * 40 + [\"RUN4\"] * 35 + [\"MITIGATIONRESULT\"] * 15 + [\"RUN1\"] * 5 + [\"RUN3\"] * 5\n",
    "CHANNELS = [\"OCEDIRECT\"] * 40 + [\"RETAIL\"] * 35 + [\"CARE\"] * 20 + [\"DIGITAL\"] * 5\n",
    "CREDIT_STATUSES = [\"APPROVED\"] * 70 + [\"DECLINED\"] * 20 + [\"PENDING\"] * 10\n",
    "DEVICE_MAKES = [\"APPLE\", \"SAMSUNG\", \"GOOGLE\", \"ONEPLUS\", \"MOTOROLA\", \"LG\"]\n",
    "MARKETS = [\"AUS\", \"DAL\", \"HOU\", \"SAT\", \"DEN\", \"PHX\", \"LAX\", \"SFO\", \"NYC\", \"CHI\", \"ATL\", \"MIA\"]\n",
    "FIRST_NAMES = [\"James\", \"Mary\", \"John\", \"Patricia\", \"Robert\", \"Jennifer\", \"Michael\", \"Linda\"]\n",
    "LAST_NAMES = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Garcia\", \"Miller\", \"Davis\"]\n",
    "\n",
    "print(f\"âœ… Reference data loaded: {len(ALL_US_STATES)} states, {len(DEVICE_MAKES)} device makes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61224d",
   "metadata": {},
   "source": [
    "## Define UDFs for Large Document Generation\n",
    "\n",
    "These UDFs create the large nested structures needed to reach 116KB per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef601e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF: Generate large WPP verification fields (~20KB)\n",
    "@udf(returnType=StringType())\n",
    "def generate_wpp_json():\n",
    "    import random, json, builtins\n",
    "    wpp = {\n",
    "        \"wppPrimaryPhoneCheckSubscriberName\": f\"User{random.randint(1000,9999)} Last{random.randint(100,999)}\",\n",
    "        \"wppSecondaryAddressCheckLinkedToPrimaryResident\": random.choice([True, False]),\n",
    "        \"wppEmailAddressCheckIsValid\": True,\n",
    "        \"wppIdentityNetworkScore\": builtins.round(random.uniform(0.5, 1.0), 3),\n",
    "        \"wppTransactionId\": f\"{random.randint(100000000, 999999999)}_{builtins.hex(random.getrandbits(128))[2:]}\",\n",
    "        \"wppPrimaryPhoneCheckCountryCode\": \"US\",\n",
    "        \"wppPrimaryPhoneCheckLineType\": random.choice([\"Landline\", \"Mobile\", \"VoIP\"]),\n",
    "        \"wppIdentityCheckScore\": random.randint(300, 900),\n",
    "        \"wppPrimaryPhoneCheckCarrier\": random.choice([\"AT&T\", \"Verizon\", \"T-Mobile\"]),\n",
    "        \"wppPrimaryAddressCheckType\": random.choice([\"Single unit\", \"Multi unit\"]),\n",
    "        \"wppIpAddressCheck\": {\n",
    "            \"zipCode\": str(random.randint(10000, 99999)),\n",
    "            \"country\": \"United States of America\",\n",
    "            \"city\": random.choice([\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"]),\n",
    "            \"countryCode\": \"US\", \"isValid\": True, \"subDivision\": \"California\", \"isProxy\": False\n",
    "        },\n",
    "        # Add 20 more fields for size\n",
    "        **{f\"wppField{i}\": f\"value_{i}_\" + \"x\"*100 for i in range(20)}\n",
    "    }\n",
    "    return json.dumps(wpp)\n",
    "\n",
    "print(\"âœ… WPP UDF defined (~20KB per document)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba76660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF: Generate large losgs array with 10-15 lineItems (~40-50KB)\n",
    "@udf(returnType=StringType())\n",
    "def generate_losgs_json():\n",
    "    import random, json, builtins\n",
    "    num_items = random.randint(10, 15)\n",
    "    line_items = []\n",
    "    for i in range(num_items):\n",
    "        line_items.append({\n",
    "            \"catalogSKUId\": f\"{random.randint(10000, 99999)}\",\n",
    "            \"displayName\": f\"PRODUCT_{i+1}_DESCRIPTION_WITH_LONG_NAME_FOR_SIZE\",\n",
    "            \"payments\": [{\"paymentTenderReference\": \"PAYMENT_1\", \"amount\": builtins.round(random.uniform(0, 500), 2)}],\n",
    "            \"hardGood\": {\"hardGoodType\": random.choice([\"SIM\", \"DEVICE\", \"ACCESSORY\"]), \"deliveryPromiseNote\": {}},\n",
    "            \"price\": {\n",
    "                \"currencyType\": \"USD\",\n",
    "                \"amount\": builtins.round(random.uniform(0, 1000), 2),\n",
    "                \"total\": builtins.round(random.uniform(0, 1000), 2),\n",
    "                \"taxDetail\": {\n",
    "                    \"taxablePriceDetail\": {\n",
    "                        \"shipToTaxAreaId\": [random.randint(100000000, 999999999) for _ in range(3)],\n",
    "                        \"taxableCost\": builtins.round(random.uniform(0, 500), 2),\n",
    "                        \"orderTaxAreaId\": random.randint(100000000, 999999999)\n",
    "                    },\n",
    "                    \"lineItemTaxes\": [{\n",
    "                        \"jurisdictionLevel\": \"SALES TAX\",\n",
    "                        \"taxCode\": \"SALESTAX\",\n",
    "                        \"taxRate\": builtins.round(random.uniform(0.05, 0.10), 4),\n",
    "                        \"taxAmount\": builtins.round(random.uniform(0, 50), 2)\n",
    "                    }]\n",
    "                },\n",
    "                \"priceType\": random.choice([\"DUE_UPON_FFL\", \"RC\", \"NRC\"])\n",
    "            },\n",
    "            \"id\": f\"LOSG-WLS-AL-001-LI-{i+1}\",\n",
    "            \"productType\": \"HARDGOOD\",\n",
    "            \"status\": random.choice([\"IN_PROGRESS\", \"COMPLETED\"])\n",
    "        })\n",
    "    \n",
    "    return json.dumps([{\n",
    "        \"dealerCode\": f\"Z{random.randint(1000, 9999)}\",\n",
    "        \"primaryIndicator\": \"TRUE\",\n",
    "        \"productCategory\": \"WIRELESS\",\n",
    "        \"lineItems\": line_items,\n",
    "        \"id\": \"LOSG-WLS-AL-001\"\n",
    "    }])\n",
    "\n",
    "print(\"âœ… losgs UDF defined (~40-50KB per document)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e50375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF: Generate neustar fraud detection data (~15-20KB)\n",
    "@udf(returnType=StringType())\n",
    "def generate_neustar_json():\n",
    "    import random, json, builtins\n",
    "    return json.dumps({\n",
    "        \"nsr\": f\"dc{random.randint(10,99)}-eid-prod-gwy{random.randint(1,9):02d}:{random.randint(100,999)}\",\n",
    "        \"transid\": f\"{builtins.hex(random.getrandbits(128))[2:]}-ocedirect\",\n",
    "        \"response\": [{\n",
    "            \"provider\": \"NEUSTAR\",\n",
    "            \"decision\": random.choice([\"APPROVE\", \"REVIEW\", \"DECLINE\"]),\n",
    "            \"score\": random.randint(0, 1000),\n",
    "            \"riskFactors\": [f\"FACTOR_{i}\" for i in range(8)],\n",
    "            \"details\": {f\"field{i}\": f\"value_{i}_\" + \"y\"*100 for i in range(15)}\n",
    "        }],\n",
    "        \"RUN4\": {\n",
    "            \"nsr\": f\"dc{random.randint(10,99)}-eid-prod-gwy02:{random.randint(100,999)}\",\n",
    "            \"response\": [{\n",
    "                \"provider\": \"NEUSTAR_RUN4\",\n",
    "                \"additionalData\": {f\"data{i}\": \"z\"*200 for i in range(10)}\n",
    "            }]\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(\"âœ… Neustar UDF defined (~15-20KB per document)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard UDFs for IDs, timestamps, etc.\n",
    "@udf(returnType=StringType())\n",
    "def generate_uuid():\n",
    "    import uuid\n",
    "    return str(uuid.uuid4()).upper()\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def generate_session_id():\n",
    "    import uuid, random\n",
    "    return f\"{random.randint(100000000, 999999999)}_{uuid.uuid4().hex}\"\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def random_datetime():\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "    base = datetime(2024, 1, 1)\n",
    "    return (base + timedelta(days=random.randint(0, 365))).isoformat() + \"Z\"\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def random_choice(choices):\n",
    "    import random\n",
    "    return random.choice(choices)\n",
    "\n",
    "print(\"âœ… Standard UDFs defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfaf29",
   "metadata": {},
   "source": [
    "## Generate Base DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de56d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸš€ Generating {NUM_RECORDS:,} base records...\")\n",
    "\n",
    "base_df = spark.range(0, NUM_RECORDS, numPartitions=NUM_PARTITIONS) \\\n",
    "    .withColumn(\"partition_id\", spark_partition_id())\n",
    "\n",
    "print(f\"âœ… Base DataFrame created with {NUM_PARTITIONS} partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c23e76",
   "metadata": {},
   "source": [
    "## Generate Large Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed803a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“¦ Building large document structure...\")\n",
    "\n",
    "# Add large JSON fields\n",
    "docs_df = base_df \\\n",
    "    .withColumn(\"wpp_json\", generate_wpp_json()) \\\n",
    "    .withColumn(\"losgs_json\", generate_losgs_json()) \\\n",
    "    .withColumn(\"neustar_json\", generate_neustar_json()) \\\n",
    "    .withColumn(\"_id\", generate_uuid()) \\\n",
    "    .withColumn(\"run\", random_choice(array([lit(r) for r in RUN_TYPES]))) \\\n",
    "    .withColumn(\"channel\", random_choice(array([lit(c) for c in CHANNELS]))) \\\n",
    "    .withColumn(\"state\", random_choice(array([lit(s) for s in WEIGHTED_STATES]))) \\\n",
    "    .withColumn(\"creditStatus\", random_choice(array([lit(cs) for cs in CREDIT_STATUSES])))\n",
    "\n",
    "# Build final document with all fields\n",
    "final_df = docs_df.select(\n",
    "    col(\"_id\"),\n",
    "    random_datetime().alias(\"applicationTimeStamp\"),\n",
    "    random_datetime().alias(\"createdDateTime\"),\n",
    "    random_datetime().alias(\"lastUpdateDateTime\"),\n",
    "    lit(\"ocedirect\").alias(\"serviceId\"),\n",
    "    generate_uuid().alias(\"version\"),\n",
    "    lit(\"com.att.bdcoe.cmt.ApplicationRecord\").alias(\"_class\"),\n",
    "    struct(\n",
    "        generate_session_id().alias(\"sessionId\"),\n",
    "        col(\"run\").alias(\"run\"),\n",
    "        col(\"channel\").alias(\"channel\"),\n",
    "        struct(\n",
    "            col(\"state\").alias(\"state\")\n",
    "        ).alias(\"shippingAddress\"),\n",
    "        col(\"creditStatus\").alias(\"creditStatus\"),\n",
    "        from_json(col(\"wpp_json\"), MapType(StringType(), StringType())).alias(\"wpp\"),\n",
    "        from_json(col(\"losgs_json\"), ArrayType(MapType(StringType(), StringType()))).alias(\"losgs\"),\n",
    "        from_json(col(\"neustar_json\"), MapType(StringType(), StringType())).alias(\"neustarDigitalIdentityRisk\")\n",
    "    ).alias(\"fields\"),\n",
    "    col(\"run\")\n",
    ").repartition(col(\"run\"))\n",
    "\n",
    "print(\"âœ… Large documents generated with ~116KB size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa88ba",
   "metadata": {},
   "source": [
    "## Verify Document Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5eda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Check actual document size\n",
    "sample = final_df.limit(1).collect()[0]\n",
    "sample_json = json.dumps(sample.asDict(recursive=True))\n",
    "doc_size_bytes = len(sample_json.encode('utf-8'))\n",
    "doc_size_kb = doc_size_bytes / 1024\n",
    "\n",
    "print(f\"ðŸ“ Actual document size: {doc_size_bytes:,} bytes ({doc_size_kb:.1f} KB)\")\n",
    "print(f\"ðŸŽ¯ Target: {TARGET_DOC_SIZE_KB} KB\")\n",
    "print(f\"âœ… Size match: {'YES' if 100 <= doc_size_kb <= 130 else 'ADJUST UDFs'}\")\n",
    "print(f\"\\nðŸ’¾ Total collection size: {NUM_RECORDS * doc_size_kb / 1024 / 1024:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5805ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data\n",
    "final_df.select(\"_id\", \"serviceId\", \"fields.run\", \"fields.channel\", \"fields.shippingAddress.state\", \"run\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f443c9",
   "metadata": {},
   "source": [
    "## Write to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019fd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ’¾ Writing to Delta Lake...\")\n",
    "\n",
    "final_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"run\") \\\n",
    "    .option(\"path\", DELTA_TABLE_PATH) \\\n",
    "    .save()\n",
    "\n",
    "print(f\"âœ… Data written to: {DELTA_TABLE_PATH}\")\n",
    "print(f\"ðŸ—‚ï¸  Partitioned by run (fields.run) for optimal query performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b0de6",
   "metadata": {},
   "source": [
    "## Verify Data and Show Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41923c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data\n",
    "delta_df = spark.read.format(\"delta\").load(DELTA_TABLE_PATH)\n",
    "total_count = delta_df.count()\n",
    "\n",
    "print(f\"âœ… Verified: {total_count:,} records written\")\n",
    "print(f\"ðŸ“¦ Total size: {total_count * doc_size_kb / 1024 / 1024:.1f} GB\")\n",
    "\n",
    "# Show distributions\n",
    "print(\"\\nðŸ“Š Distribution by run type (PARTITION KEY):\")\n",
    "delta_df.groupBy(\"run\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\nðŸ“Š Distribution by channel:\")\n",
    "delta_df.groupBy(\"fields.channel\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\nðŸ“Š Distribution by credit status:\")\n",
    "delta_df.groupBy(\"fields.creditStatus\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"\\nðŸ“Š Distribution by state (top 10):\")\n",
    "delta_df.groupBy(\"fields.shippingAddress.state\").count().orderBy(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d906d7",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary\n",
    "\n",
    "### âœ… Generated Data:\n",
    "- **Document size**: ~116 KB per document (matches customer sample)\n",
    "- **Total records**: As configured in NUM_RECORDS\n",
    "- **Partitioned by**: `run` (fields.run) for optimal query performance\n",
    "\n",
    "### ðŸ“¦ Document Components:\n",
    "- **Top-level fields**: _id, serviceId, timestamps, version\n",
    "- **fields.run**: RUN2, RUN4, MITIGATIONRESULT (weighted distribution)\n",
    "- **fields.channel**: OCEDIRECT, RETAIL, CARE, DIGITAL\n",
    "- **fields.shippingAddress.state**: All 51 US states (TX, CA weighted higher)\n",
    "- **fields.creditStatus**: APPROVED, DECLINED, PENDING\n",
    "- **wpp**: ~20KB nested JSON structure\n",
    "- **losgs**: ~40-50KB array of nested objects\n",
    "- **neustarDigitalIdentityRisk**: ~15-20KB nested JSON\n",
    "\n",
    "### ðŸŽ¯ Partition Key Strategy:\n",
    "- **Partition by**: fields.run (4-6 logical partitions)\n",
    "- **Query pattern**: 95% of queries filter by run type\n",
    "- **Distribution**: RUN2 (~50%), RUN4 (~35%), MITIGATIONRESULT (~15%)\n",
    "- **Performance**: Optimal for avetrack query patterns from queries.txt\n",
    "\n",
    "### ðŸ“Š Next Steps:\n",
    "1. Verify document size (~116KB)\n",
    "2. Check partition distribution by run type\n",
    "3. Export to MongoDB/DocumentDB for performance testing\n",
    "4. Run queries from mongodb_avetrack_queries.json\n",
    "5. Create indexes from mongodb_avetrack_indexes.json"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
